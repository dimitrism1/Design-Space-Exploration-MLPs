{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac052873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 19:57:19.168725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 19:57:19.254690: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c1b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitsas/anaconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights, load_qmodel\n",
    "import hls4ml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31565857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/models_2/prec_8/model_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_RF = 100\n",
    "id_num = 2\n",
    "precision = 8\n",
    "dirname = './models/models_' + str(id_num) + \"/prec_\" + str(precision) + \"/model_\"\n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906bc2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 13:15:01.332236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmitsas/anaconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "layer = np.arange(3,23)\n",
    "model_num = 20\n",
    "\n",
    "models = []\n",
    "bits = 8\n",
    "int_bits = 0\n",
    "for i in range(model_num):\n",
    "    model=Sequential()\n",
    "    model.add(QDense(layer[i], input_shape=(21,), name='fc1', kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "    model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu1'))\n",
    "    model.add(QDense(3, name='output',\n",
    "                    kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "    model.add(Activation(activation='softmax', name='softmax'))\n",
    "    models.append(model)\n",
    "#model.layers[0].set_weights(wb1)\n",
    "#model.layers[2].set_weights(wb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5eb3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n",
      "... quantizing model\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "import qkeras.qtools.qtools_util\n",
    "import qkeras.estimate\n",
    "loaded_models = []\n",
    "for model_no in range(model_num):\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    loaded_models.append(load_model(dirname + str(model_no) + '/KERAS_check_best_model.h5', custom_objects=co))\n",
    "for i in loaded_models:\n",
    "    qkeras.utils.get_model_sparsity(i)\n",
    "#model = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7500f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Train = False\n",
    "selector = 0\n",
    "for precision in range(8,1,-1):\n",
    "    dirname = './models/models_' + str(id_num) + \"/prec_\" + str(precision) + \"/model_\"\n",
    "    for i in range(0,model_num,1):    \n",
    "        adam = Adam(lr=0.02)\n",
    "        if(Train):\n",
    "            pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.2, begin_step=2000, frequency=100)}\n",
    "            models[i] = prune.prune_low_magnitude(models[i], **pruning_params)\n",
    "            models[i].compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "            callbacks= all_callbacks( outputDir = 'arrythmia_classification_prune_new')\n",
    "            callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "    #         models[i].fit(X_train, Y_train, batch_size=1,\n",
    "    #                   epochs=15,validation_split=0.2, verbose=1, shuffle=True,\n",
    "    #                   callbacks = callbacks.callbacks);\n",
    "            models[i].fit(X_train,Y_train,batch_size=1024,epochs=10,\n",
    "                validation_split=0.25,\n",
    "                verbose=1,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks.callbacks,\n",
    "                )\n",
    "            models[i] = strip_pruning(models[i])\n",
    "            #model_save_quantized_weights(models[i], \"test_weights_new_\" + str(i) + \"_\" + str(id_num))\n",
    "            models[i].save(dirname + str(i) + '/KERAS_check_best_model.h5')\n",
    "\n",
    "                #stripped_model = strip_pruning(models[i])\n",
    "                #stripped_model.save('models/stripped_models' + str(id_num) + '/stripped_model'+str(i)+'/KERAS_check_best_model.h5')\n",
    "        for j in range(0,10,1):\n",
    "            loaded_models[i].save(dirname + str(i) + '/KERAS_check_best_model.h5')\n",
    "            config = hls4ml.utils.config_from_keras_model(loaded_models[i],granularity='name')\n",
    "            #config['Model']['Precision']='fixed<16,4>'\n",
    "            #config['LayerName']['fc1_input']['Precision']['result'] = 'fixed<8,2>'\n",
    "            #config['LayerName']['output']['Precision']['result'] = 'fixed<16,4>'\n",
    "            config['LayerName']['fc1_input']['Precision']['result'] = 'fixed<' + str(precision) + ',2>'\n",
    "            config['LayerName']['relu1']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            \n",
    "            config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "            config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "            config['Model']['ReuseFactor'] = j + 1\n",
    "            #config['LayerName']['fc1']['Precision']['result'] = 'fixed<16,4>'\n",
    "            #config['LayerName']['fc2']['Precision']['result'] = 'fixed<16,4>'\n",
    "\n",
    "            hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "                loaded_models[i],hls_config = config,output_dir = dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj',part = 'xc7z007s-clg225-2')\n",
    "            hls_model.compile()\n",
    "            os.system('cp -r build_prj_orig.tcl project_orig.tcl build_prj_orig_lut.tcl ' + dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            os.system('cp init.sh init_lut.sh ' + dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            start_dir = os.getcwd()\n",
    "            os.chdir(r'./models/models_'+ str(id_num) + \"/prec_\" + str(precision) + \"/model_\" + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            if selector:\n",
    "                os.system('bash init.sh')\n",
    "                os.system('rm init_lut.sh')\n",
    "            else:\n",
    "                os.system('rm init.sh')\n",
    "                os.system('bash init_lut.sh')\n",
    "            os.chdir(start_dir)        \n",
    "            if j==9:\n",
    "                     for num in range(j+2):\n",
    "                         os.system('rm -r ' + dirname +str(i) + '/reuse_' + str(num) + '/hls4ml_prj/myproject_prj/solution1/.autopilot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e860e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "selector = 0\n",
    "for precision in range (8,1,-1):\n",
    "    dirname = './models/models_' + str(id_num) + \"/prec_\" + str(precision) + \"/model_\"\n",
    "    for i in range(0,model_num,1):\n",
    "        for j in range(19,max_RF,10):\n",
    "            config = hls4ml.utils.config_from_keras_model(loaded_models[i],granularity='name')\n",
    "            config['LayerName']['fc1_input']['Precision']['result'] = 'fixed<' + str(precision) + ',2>'\n",
    "            config['LayerName']['relu1']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            #config['LayerName']['relu2']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            #config['LayerName']['relu3']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            \n",
    "            config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "            config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "            config['Model']['ReuseFactor'] = j + 1\n",
    "                #config['LayerName']['fc1']['Precision']['result'] = 'fixed<16,4>'\n",
    "                #config['LayerName']['fc2']['Precision']['result'] = 'fixed<16,4>'\n",
    "\n",
    "            hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "            loaded_models[i],hls_config = config,output_dir = dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj',part = 'xc7z007s-clg225-2')\n",
    "            hls_model.compile()\n",
    "            os.system('cp -r build_prj_orig.tcl project_orig.tcl build_prj_orig_lut.tcl ' + dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            os.system('cp init.sh init_lut.sh ' + dirname + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            start_dir = os.getcwd()\n",
    "            os.chdir(r'./models/models_'+ str(id_num) + \"/prec_\" + str(precision) + \"/model_\" + str(i) + '/reuse_' + str(j + 1) + '/hls4ml_prj')\n",
    "            if selector:\n",
    "                os.system('bash init.sh')\n",
    "                os.system('rm init_lut.sh')\n",
    "            else:\n",
    "                os.system('rm init.sh')\n",
    "                os.system('bash init_lut.sh')\n",
    "            os.chdir(start_dir)\n",
    "            if j==99:\n",
    "                    for num in range(j + 2):\n",
    "                        os.system('rm -r ' + dirname + str(i) + '/reuse_' + str(num) + '/hls4ml_prj/myproject_prj/solution1/.autopilot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474acdde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
