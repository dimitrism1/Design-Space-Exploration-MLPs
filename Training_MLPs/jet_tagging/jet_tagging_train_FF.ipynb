{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f7ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights, load_qmodel\n",
    "import hls4ml\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337639c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e3c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8d94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4cdb1",
   "metadata": {},
   "source": [
    "## Construct a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf5550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/models_3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_num = 3\n",
    "\n",
    "#dirname = './models/models_' + str(id_num) + \"/model_\"\n",
    "dirname = './models/models_' + str(id_num) \n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8584a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_RF = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d9381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### if the layers are already generated the ready flag should be set to False in order to load them ######\n",
    "\n",
    "ready = True\n",
    "models = []\n",
    "sec_layer = []\n",
    "third_layer = []\n",
    "fourth_layer = []\n",
    "model_num = 20\n",
    "model_layers = [[] for x in range(model_num)]\n",
    "bits = 8\n",
    "int_bits = 0\n",
    "for i in range(model_num):\n",
    "    if ready:\n",
    "        layer_1 = 16\n",
    "        layer_2 = random.randint(64,100)                           ###64\n",
    "        sec_layer.append(layer_2)\n",
    "        layer_3 = random.randint(32,64)                           ###32\n",
    "        third_layer.append(layer_3)\n",
    "        layer_4 = random.randint(16,32)                           ###32\n",
    "        fourth_layer.append(layer_4)\n",
    "        layer_5 = 5\n",
    "        model_layers[i].append(layer_2)\n",
    "        model_layers[i].append(layer_3)\n",
    "        model_layers[i].append(layer_4)\n",
    "        if not (os.path.exists('layerdetails/models_' + str(id_num) + '/model_' + str(i))):\n",
    "            os.makedirs('layerdetails/models_' + str(id_num) + '/model_' + str(i))\n",
    "        with open('layerdetails/models_' + str(id_num) + '/model_' + str(i) + \"/layers.txt\",'w') as f:\n",
    "            f.write(str(layer_2) + \" \" + str(layer_3) + \" \" + str(layer_4))\n",
    "        f.close\n",
    "        #np.savetxt(dirname + str(i) + '/layerdetails.txt',model_layers[i],delimiter=\"\\n\" )\n",
    "    else:\n",
    "        with open('layerdetails/models_' + str(id_num) + '/model_' + str(i) + \"/layers.txt\",'r') as r:\n",
    "            #print(r.readlines()[0])\n",
    "            layer = r.readlines()[0].split(\" \")\n",
    "            layer_1 = 16\n",
    "            layer_2 = int(layer[0])\n",
    "            layer_3 = int(layer[1])\n",
    "            layer_4 = int(layer[2])\n",
    "            layer_5 = 5\n",
    "            sec_layer.append(layer_2)\n",
    "            third_layer.append(layer_3)\n",
    "            fourth_layer.append(layer_4)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(QDense(layer_2, input_shape=(layer_1,), name='fc1', kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "    model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu1'))\n",
    "    model.add(QDense(layer_3, name='fc2',\n",
    "                    kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "    model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu2'))\n",
    "    model.add(QDense(layer_4, name='fc3',\n",
    "                    kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "    model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu3'))\n",
    "    model.add(QDense(layer_5, name='output',\n",
    "                    kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                    kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "    model.add(Activation(activation='softmax', name='softmax'))\n",
    "    models.append(model)\n",
    "model_layers = np.array(model_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58d12c",
   "metadata": {},
   "source": [
    "# Train for different precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5f3ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### load pre-trained models ######\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "import qkeras.qtools.qtools_util\n",
    "import qkeras.estimate\n",
    "loaded_models = []\n",
    "pre = 8\n",
    "id_num = 2\n",
    "model_num = 20\n",
    "dirname = './models/models_' + str(id_num) \n",
    "for model_no in range(model_num):\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    loaded_models.append(load_model(dirname + \"/prec_\" + str(pre) + \"/model_\" + str(model_no) + '/KERAS_check_best_model.h5', custom_objects=co))\n",
    "    qkeras.utils.get_model_sparsity(loaded_models[model_no])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43696684",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_num = 3\n",
    "\n",
    "#dirname = './models/models_' + str(id_num) + \"/model_\"\n",
    "dirname = './models/models_' + str(id_num) \n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794b1a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##### If no pre-trained models are present the Train flag should be set to true\n",
    "Train = False\n",
    "selector = 1\n",
    "for precision in range(8,1,-1):\n",
    "    dirname = './models/models_' + str(id_num) + \"/prec_\" + str(precision) + \"/model_\"\n",
    "    for i in range(0,model_num,1):    \n",
    "        adam = Adam(lr=0.02)\n",
    "        if(Train):\n",
    "            loaded_models = models\n",
    "            pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.2, begin_step=2000, frequency=100)}\n",
    "            loaded_models[i] = prune.prune_low_magnitude(loaded_models[i], **pruning_params)\n",
    "            loaded_models[i].compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "            callbacks= all_callbacks( outputDir = 'jet_tagging_new')\n",
    "            callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "    #         models[i].fit(X_train, Y_train, batch_size=1,\n",
    "    #                   epochs=15,validation_split=0.2, verbose=1, shuffle=True,\n",
    "    #                   callbacks = callbacks.callbacks);\n",
    "            loaded_models[i].fit(X_train,Y_train,batch_size=1024,epochs=10,\n",
    "                validation_split=0.25,\n",
    "                verbose=1,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks.callbacks,\n",
    "                )\n",
    "            loaded_models[i] = strip_pruning(models[i])\n",
    "            #model_save_quantized_weights(models[i], \"test_weights_new_\" + str(i) + \"_\" + str(id_num))\n",
    "            loaded_models[i].save(dirname + str(i) + '/KERAS_check_best_model.h5')\n",
    "\n",
    "                #stripped_model = strip_pruning(models[i])\n",
    "                #stripped_model.save('models/stripped_models' + str(id_num) + '/stripped_model'+str(i)+'/KERAS_check_best_model.h5')\n",
    "        rf_range = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "        for j in rf_range:\n",
    "            config = hls4ml.utils.config_from_keras_model(loaded_models[i],granularity='name')\n",
    "            config['LayerName']['fc1_input']['Precision']['result'] = 'fixed<' + str(precision) + ',2>'\n",
    "            config['LayerName']['relu1']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            config['LayerName']['relu2']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            config['LayerName']['relu3']['Precision']['result'] = 'ap_ufixed<' + str(precision) + ',0,AP_RND_CONV,AP_SAT>'\n",
    "            \n",
    "            config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "            config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "            config['Model']['ReuseFactor'] = j\n",
    "            #config['LayerName']['fc1']['Precision']['result'] = 'fixed<16,4>'\n",
    "            #config['LayerName']['fc2']['Precision']['result'] = 'fixed<16,4>'\n",
    "\n",
    "            hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "                loaded_models[i],hls_config = config,output_dir = dirname + str(i) + '/reuse_' + str(j) + '/hls4ml_prj',part = 'xc7z007s-clg225-2')\n",
    "            hls_model.compile()\n",
    "            os.system('cp -r build_prj_orig.tcl project_orig.tcl build_prj_orig_lut.tcl ' + dirname + str(i) + '/reuse_' + str(j) + '/hls4ml_prj')\n",
    "            os.system('cp init.sh init_lut.sh ' + dirname + str(i) + '/reuse_' + str(j) + '/hls4ml_prj')\n",
    "            start_dir = os.getcwd()\n",
    "            os.chdir(r'./models/models_'+ str(id_num) + \"/prec_\" + str(precision) + \"/model_\" + str(i) + '/reuse_' + str(j) + '/hls4ml_prj')\n",
    "            if selector:\n",
    "                os.system('bash init.sh')\n",
    "                os.system('rm init_lut.sh')\n",
    "            else:\n",
    "                os.system('rm init.sh')\n",
    "                os.system('bash init_lut.sh')\n",
    "            os.chdir(start_dir)        \n",
    "            if j==100:\n",
    "                     for num in range(j+2):\n",
    "                         os.system('rm -r ' + dirname +str(i) + '/reuse_' + str(num) + '/hls4ml_prj/myproject_prj/solution1/.autopilot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bc85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
