## Process description
The design space explorations consists of two parts. Creating an array for the accuracy of a model and then the same for the resources of this model. The resources are gathered into a single normalised metric. In the problem class the evaluate method is used to generate random parameter values. These values are then fed into the create_pop function which, creates a neural network with the given data, configures it as an HLS model and then calculates the accuracy of the model on the given test data. Those values are then stored into F1. The solutions are then analyzed and the optimal solutions with regards to the accuracy and the resources of the models.

The estimator is used to estimate the resources for each model and the generated parameters and then fed into F2. These two arrays are the output of the NSGA-II and create the solution set

More on genetic algorithms and pymoo: https://pymoo.org/
### Usage of the dse class
The function that handles the post processing data has to be given as an argument to the dse class. In the Algorithm notebook an example is provided to show how it is done for a classification problem.

First the directory where the results will be saved, is specified as filename along with the desired population size. The inputs and outputs of the neural network are also specified in the beginning. The problem is then instantiated as a dse object, which includes all the arguments for the class. The size of the input and output layers, the training and testing data, the bath size the post processing function and the preferred device. In addition the flag only_est decides if only the estimator will run or the whole process including the crp function. This can be particurarly helpful when we change the RF and only need to rerun the estimator part of the class to determine the new solutions, as RF doesn't have an impact on accuracy

### If the solutions and parameters are already available

If there isn't a need to create the solutions set, but it is readily available, then there isn't a need to utilize the create_pop function to train new models and make predictions to determine the HLS model accuracy. To find the Pareto front the same arguments are fed into the dse class along with the starting population and the parameters of this population. If a population is given the dse class doesn't run the the create_pop function but uses the NSGA-II on the given solutions set

### Getting the results

The parameters and solutions can be retrieved to find out which sets were generated by the evaluation function. By using the get_opt_solutions and get_opt_params the Pareto front solutions are printed whereas the fitting solutions show which solutions can fit into the selected device(resources<1). The pareto front can also be plotted

